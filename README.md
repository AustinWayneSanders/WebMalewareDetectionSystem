# WebMalewareDetectionSystem: Moving Towards a Java Framework with Automation

>The purpose of this project is to develop a classification model that determines whether a website is either malicious or benign. Detecting malicious websites can be advantageous in cyber security as the web “has been one of the most effective mechanism for criminals to distribute malicious code” [1]. Today’s web possess many challenges in the dependency of the data being analyzed [2]. As webpages become more dynamic in state changes, current detection algorithms may overlook potentially malicious websites. This project may offer novel solutions regarding dynamically evolving websites by analyzing elements that may elicit malicious state changes (such as DOM-modifying methods/functions), allowing malware to infect the host. The following are a few common challenges with current malicious websites [2]: state explosion, state navigation, triggering state changes, and unreachable states.
>
>Particularly, this project explores the similarities of tokenized website HTML document characters following the classical term frequency inverse document frequency (TF-IDF) algorithm, using a text vector space. The TF-IDF algorithm is common method as an information retrieval and extraction subtask used to support content analysis of web pages for text categorization [3]–[5]. Furthermore, the TF-IDF algorithm takes a ‘bag of words’ approach [4].  The ‘bag of words’ is lazy learning method that does not assume any sequential relationship among tokenized elements. The ‘bag of words’ is often used with k-nearest neighbors, as text categorization is generally a supervised classification method where known labels of text categorized are trained to predict unknown labels of newly retrieved documents [6]. Moreover, TF-IDF is a statistical technique that identifies strong words from a corpus and gives it a larger weight than words that are common [3]. In turn, this aids distance measures in distinguishing dissimilarities in text categorizations.  

